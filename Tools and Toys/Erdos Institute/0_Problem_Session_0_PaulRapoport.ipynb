{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra and Multivariable Calculus Self Assessment\n",
    "\n",
    "You can use numpy (or other software which supports linear algebra) to assist with any linear algebra computations.  \n",
    "\n",
    "If you are *extremely* sure that you can do these computations, you should just skim over the solutions and confirm that they make sense to you.  If you are confident that you could produce solutions which are of comparable or superior quality then the Linear Algebra and Multivariable Calculus parts of the Crash Course are probably not worth your time!\n",
    "\n",
    "1. Compute the total derivative $Df$ of the function $f: \\mathbb{R}^2 \\to \\mathbb{R}^3$ defined by $f(x,y) = (xy, x, x^2-y^2)$.  Use the linear approximation of $f$ at $(1,2)$ to approximate $f(1.01, 1.99)$.\n",
    "2. Consider the function $f: \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $f(x,y,z) = x^2 + y^2 +z^2 - xy - 2x + 3y + z + 1$. Find the stationary points of $f$. Determine whether the Hessian is positive definite, positive semi-definite, negative semi-definite, negative definite, or none of the above.  What does this tell you about the stationary points:  are they local maxima, local minima, or saddle points?\n",
    "\n",
    "* Note:  The first two questions deal with multivariable differential calculus and optimization.  Many data science techniques involve specifying a model as a function of some parameters, and minimizing a loss function with respect to these parameters to \"fit\" the model.  An understanding of multivariate differential calculus is essential for really understanding what is happening with these techniques.  For instance: linear regression, logistic regression, support vector machines, and training a neural network all depend on minimizing a loss function of some parameters.\n",
    "\n",
    "3.  Project the vector $ \\vec{y} = \\begin{bmatrix}  1 \\\\ 2 \\\\ 3\\end{bmatrix}$ orthogonally onto the image of the linear transformation with matrix $A = \\begin{bmatrix} 1 & 2 \\\\ 1 & 3 \\\\ 1 & -1  \\end{bmatrix}$.  \n",
    "\n",
    "4. Find the Singular Value Decomposition of the matrix $\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6\\end{bmatrix}$.  Use the SVD to find a rank 1 approximation of the matrix.\n",
    "\n",
    "* Note:  Singular Value Decomposition is essential for understanding principle component analysis, which is an important dimension reduction technique often used in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paul Rapoport's Answers:\n",
    "\n",
    "1. \n",
    "$$\n",
    "\\begin{align*}\n",
    "Df &= [[\\partial f_1/\\partial x, \\partial f_1/\\partial y], [\\partial f_2/\\partial x, \\partial f_2/\\partial y], [\\partial f_3/\\partial x, \\partial f_3/\\partial y]]\\\\\n",
    "&= [[y, x],[1, 0],[2x, -2y]]\\\\\n",
    "\n",
    "L_{(1, 2)}f(x) &= f(1, 2) + Df(1, 2) \\cdot (x - (1, 2))\\\\\n",
    "&= [2, 1, -3] + [[2, 1],[1, 0],[4, -2]] \\cdot [0.01, -0.01]\\\\\n",
    "&= [2, 1, -3] + [0.02 - 0.01, 0.01 + 0, 0.04 + 0.02]\\\\\n",
    "&= [1.99, 0.99, -2.94]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. We want: $ \\partial f / \\partial x = \\partial f / \\partial y = \\partial f / \\partial z = 0 $. Calculating this out, we have $ \\partial f / \\partial x = 2x - y - 2, \\partial f / \\partial y = 2y - x + 3, \\partial f / \\partial z = 2z + 1$. Then $ z = \\frac{1}{2}$, and we have a system of equations $2x - y = 2, -x + 2y = -3$, which has solution $x = \\frac{1}{3}, y = -\\frac{4}{3}$, that is, the unique stationary point is at $(\\frac{1}{3}, -\\frac{4}{3}, \\frac{1}{2})$.\\\\\n",
    "We form the Hessian $\\mathcal{H}f$ as the matrix of all possible second derivatives, which will be symmetric; it thus suffices to check the eigenvalues of $\\mathcal{H}f(1, 2)$ in order to check for sign-definiteness; in fact, since all second partial derivatives of $f$ are constant, it suffices to check this for the (constant) general Hessian. We have:\n",
    "$$\n",
    "\\mathcal{H}f =\n",
    "\\begin{bmatrix}\n",
    "2 & -1 & 0 \\\\\n",
    "-1 & 2 & 0 \\\\\n",
    "0 & 0 & 2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "so that eigenvalues are given by solutions to\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\big\\vert \\mathcal{H}f - \\lambda I \\big\\vert &= 0\\\\\n",
    "(2-\\lambda)((2-\\lambda)^2 - 1) &= 0\\\\\n",
    "(2-\\lambda)(2 - 2\\lambda + \\lambda^2 - 1) = (2-\\lambda)(3 - 2\\lambda + \\lambda^2) &= 0\\\\\n",
    "(2-\\lambda)(1 - \\lambda)(3 - \\lambda) &= 0,\n",
    "\\end{align*}\n",
    "$$\n",
    "that is, $ \\lambda \\in \\{1, 2, 3\\} $. We could solve for the eigenvectors now if we chose, but we don't have to; we already have enough to know that the Hessian is positive-definite and thus that our stationary point is a local (and probably global) minimum.\n",
    "\n",
    "3. For projection vector $\\vec{p}_y$, we want for two things to be true: that for some $\\vec{x} \\in \\mathbb{R}^2$, $A\\vec{x} = \\vec{p}_y$, and that $A^T (\\vec{y} - \\vec{p}_y) = \\vec{0}$, that is, that $\\vec{p}_y$ is in the image of the (invertible) linear map that $A$ represents, and that $(y - \\vec{p}_y)$ is orthogonal to the image of $A$ (this last because $A^TA\\vec{v} = A^T \\vec{w}$ so that a vector being orthogonal to the image of $A$ is the same as $A^T$ taking the vector to the zero vector). Thus we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "A^T (\\vec{y} - A\\vec{x}) &= \\vec{0}\\\\\n",
    "A^T \\vec{y} - A^TA\\vec{x} &= \\vec{0}\\\\\n",
    "A^TA\\vec{x} &= A^T \\vec{y}\\\\\n",
    "\\vec{x} &= (A^TA)^{-1}A^T \\vec{y}\\\\\n",
    "A\\vec{x} &= A(A^TA)^{-1}A^T \\vec{y}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23076923  0.07692308  0.69230769]\n",
      " [ 0.07692308  0.19230769 -0.26923077]] (2, 3)\n",
      "[ 2.46153846 -0.34615385] (2,)\n",
      "p_y = [1.76923077 1.42307692 2.80769231]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "mat_A = np.array([[1, 2], [1, 3], [1, -1]])\n",
    "vec_y = np.array([1, 2, 3]).T\n",
    "\n",
    "# print(np.shape(np.matmul(linalg.inv(np.matmul(mat_A.T, mat_A)), mat_A.T)))\n",
    "\n",
    "proj_mat = np.matmul(linalg.inv(np.matmul(mat_A.T, mat_A)), mat_A.T)  # important to use matmul here - the default is elementwise! \n",
    "print(proj_mat, np.shape(proj_mat))\n",
    "vec_x = np.matmul(proj_mat, vec_y)\n",
    "print(vec_x, np.shape(vec_x))\n",
    "proj_vec = np.matmul(mat_A, vec_x)\n",
    "print('p_y =', proj_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Let $$ B = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \n",
    "\\end{bmatrix}. $$\n",
    "We want to find the SVD of $B = U\\Sigma V^T$. To do this, we'll start by finding the eigenvalues of $BB^T$ and take their square roots, which will give us $\\Sigma$. We have\n",
    "$$\n",
    "BB^T=\n",
    "\\begin{bmatrix}\n",
    "5 & 11 & 17 \\\\\n",
    "11 & 25 & 39 \\\\\n",
    "17 & 39 & 61 \n",
    "\\end{bmatrix}\n",
    "\n",
    "(maybe I want to do #4 primarily in code instead?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Self Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Compute the total derivative $Df$ of the function $f: \\mathbb{R}^2 \\to \\mathbb{R}^3$ defined by $f(x,y) = (xy, x, x^2-y^2)$.  Use the linear approximation of $f$ at $(1,2)$ to approximate $f(1.01, 1.99)$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Df\\big\\vert_{(x,y)} \n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial}{\\partial x} (xy) & \\frac{\\partial}{\\partial y} (xy) \\\\\n",
    "\\frac{\\partial}{\\partial x} (x) & \\frac{\\partial}{\\partial y} (x) \\\\\n",
    "\\frac{\\partial}{\\partial x} (x^2 - y^2) & \\frac{\\partial}{\\partial y} (x^2 - y^2) \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "y & x \\\\\n",
    "1 & 0 \\\\\n",
    "2x & -2y \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "Df\\big\\vert_{(1,2)} \n",
    "= \n",
    "\\begin{bmatrix} \n",
    "2 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & -4 \\\\\n",
    "\\end{bmatrix}\\\\ \n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(1.01, 1.99) \n",
    "&= f( (1,2) + \\begin{bmatrix} 0.01 \\\\ -0.01\\end{bmatrix})\\\\\n",
    "&\\approx f(1,2) + Df\\big\\vert_{(1,2)}\\begin{bmatrix} 0.01 \\\\ -0.01\\end{bmatrix}\\\\\n",
    "&= (2,1,-3) + \\begin{bmatrix} \n",
    "2 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & -4 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix} 0.01 \\\\ -0.01\\end{bmatrix}\\\\\n",
    "&=(2,1,-3) + \\begin{bmatrix}0.01 \\\\ 0.01\\\\ -0.02 \\end{bmatrix}\\\\\n",
    "&= (2.01, 1.01, -2.94)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can check this against the exact value $f(1.01, 1.99) = (2.0099, 1.01, -2.94)$ which is very close!\n",
    "\n",
    "> 2. Consider the function $f: \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $f(x,y,z) = x^2 + y^2 +z^2 - xy - 2x + 3y + z + 1$. Find the stationary points of $f$. Determine whether the Hessian is positive definite, positive semi-definite, negative semi-definite, negative definite, or none of the above.  What does this tell you about the stationary points:  are they local maxima, local minima, or saddle points?\n",
    "\n",
    "\n",
    "The gradient of $f$ is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla f &= \\begin{bmatrix}  \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\\\ \\frac{\\partial f}{\\partial z}\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix} 2x - y - 2 \\\\ 2y - x + 3  \\\\ 2z + 1\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So the gradient is 0 when \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "2x - y = 2 \\\\ 2y - x   = -3 \\\\ 2z =  -1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We could solve this system by hand, but I will use numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stationary point is \n",
      " [[ 0.33333333]\n",
      " [-1.33333333]\n",
      " [-0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "M = np.array([[2, -1, 0 ],[-1, 2, 0],[0, 0, 2]])\n",
    "stationary_point = np.dot(linalg.inv(M), [[2],[-3],[-1]])\n",
    "print('The stationary point is \\n', stationary_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian matrix is \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{H}f \n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial z \\partial x}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} & \\frac{\\partial^2 f}{\\partial y^2} & \\frac{\\partial^2 f}{\\partial z \\partial y}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial z} & \\frac{\\partial^2 f}{\\partial y \\partial z} & \\frac{\\partial^2 f}{\\partial z^2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\begin{bmatrix} \n",
    "2 & -1 & 0\\\\\n",
    "-1 & 2 & 0\\\\\n",
    "0 & 0 & 2\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We need to check the definiteness of this matrix.\n",
    "\n",
    "This is actually easy enough to do by inspection:  \n",
    "\n",
    "* $\\begin{bmatrix} 0 \\\\ 0 \\\\ 1\\end{bmatrix}$ is an eigenvector with eigenvalue $2$\n",
    "* $\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$ is an eigenvector with eigenvalue $1$. \n",
    "\n",
    " Since the trace is $6$, we must have another eigenvector with eigenvalue $3$, so this matrix is positive definite.  Thus the critical point $(\\frac{1}{3}, -\\frac{4}{3}, -\\frac{1}{2})$ is where the global minimum value of this function occurs.  \n",
    "\n",
    "Addendum:  Oh, I see it now:  the eigenvector $\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$ has eigenvalue $3$.  Actually, we should have anticipated this, since by the real spectral theorem the eigenvectors of a symmetric matrix must be orthogonal.  Thinking geometrically, this vector must have been in the $xy$ plane to be orthogonal to the first eigenvector, and then to be perpendicular to $\\begin{bmatrix} 1 & 1 & 0 \\end{bmatrix}^\\top$ it had to be $\\begin{bmatrix} 1 & -1 & 0 \\end{bmatrix}^\\top$!\n",
    "\n",
    "Let's double check ourselves with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues are  [3. 1. 2.]\n",
      "The eigenvectors are the columns of \n",
      " [[ 0.70710678  0.70710678  0.        ]\n",
      " [-0.70710678  0.70710678  0.        ]\n",
      " [ 0.          0.          1.        ]]\n",
      "These are the same eigenvectors we found, just normalized\n"
     ]
    }
   ],
   "source": [
    "H = np.array([[2,-1,0],[-1,2,0],[0,0,2]])\n",
    "\n",
    "# linalg.eig is a function which takes a matrix and returns a tuple of arrays. The index 0 element of the tuple is the array of eigevalues.\n",
    "# the index 1 tuple is the matrix whose columns are eigenvectors.\n",
    "\n",
    "print('The eigenvalues are ', linalg.eig(H)[0])\n",
    "print('The eigenvectors are the columns of \\n', linalg.eig(H)[1])\n",
    "print('These are the same eigenvectors we found, just normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3.  Project the vector $ \\vec{y} = \\begin{bmatrix}  1 \\\\ 2 \\\\ 3\\end{bmatrix}$ onto the image of the linear transformation with matrix $A = \\begin{bmatrix} 1 & 2 \\\\ 1 & 3 \\\\ 1 & -1  \\end{bmatrix}$\n",
    "\n",
    "There are several approaches.  In this case, when the dimension of the image is low, it is not unreasonable to use the Gram–Schmidt process to find an orthonormal basis of the image, and then project $\\vec{y}$ onto the span of each basis vector.\n",
    "<br>\n",
    "I will take a different approach, which gives an explicit formula for the projection in terms of $A$ and $\\vec{y}$.  \n",
    "<br>\n",
    "Call the projected vector $\\vec{p}$.  Then we want $\\vec{p}$ to be in the image of $A$ so $\\vec{p} = A\\vec{x}$ for some $\\vec{x} \\in \\mathbb{R}^2$.  We also want $\\vec{y} - \\vec{p}$ to be perpedicular to the image of $A$.  So we want $A^\\top (\\vec{y} - \\vec{p}) = \\vec{0}$.\n",
    "<ve>\n",
    "Putting this together we have \n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "& A^\\top (\\vec{y} - A\\vec{x}) = \\vec{0}\\\\\n",
    "& A^\\top A\\vec{x} = A^\\top \\vec{y}\\\\\n",
    "& \\vec{x} = (A^\\top A)^{-1} (A^\\top \\vec{y}), \\textrm{see Note}\\\\\n",
    "& A \\vec{x} = A (A^\\top A)^{-1} (A^\\top \\vec{y})\\\\\n",
    "& p = A (A^\\top A)^{-1} (A^\\top \\vec{y})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note 1:  $(A^\\top A)$ is invertible when $A$ has linearly independent columns, which holds in our example.\n",
    "\n",
    "Note 2:  This is the \"normal equation\" for least squares linear regression!  We will see that connection spelled out in a later notebook.\n",
    "\n",
    "We could do this by hand since $A$ is just a $3 \\times 2$ matrix, but I will use numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.76923077]\n",
      " [1.42307692]\n",
      " [2.80769231]]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[1],[2],[3]])\n",
    "A = np.array([[1,2],[1,3],[1,-1]])\n",
    "Aty = np.dot(A.transpose(),y) # A-tranpose times y\n",
    "AtAinv = linalg.inv(np.dot(A.transpose(), A)) # the inverse of A-tranpose times A.\n",
    "p = np.dot(A, np.dot(AtAinv, Aty)) # writing out the formula we found.\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0, -0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check that y - p is perpendicular to both columns of A.  It is clearly in the image of A since it is defined as A applied to a vector.\n",
    "# Rounded to 10 place values, these dot products are both 0.\n",
    "\n",
    "round(np.dot(A[:,0].reshape(1,3),(y-p))[0,0],10), round(np.dot(A[:,1].reshape(1,3),(y-p))[0,0],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Find the singular value decomposition of the matrix $A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6\\end{bmatrix}$.  Use the SVD to find a rank 1 approximation of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2],[3,4],[5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVDResult(U=array([[-0.2298477 ,  0.88346102,  0.40824829],\n",
       "       [-0.52474482,  0.24078249, -0.81649658],\n",
       "       [-0.81964194, -0.40189603,  0.40824829]]), S=array([9.52551809, 0.51430058]), Vh=array([[-0.61962948, -0.78489445],\n",
       "       [-0.78489445,  0.61962948]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = \n",
      " [[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "S = \n",
      " [[9.52551809 0.        ]\n",
      " [0.         0.51430058]\n",
      " [0.         0.        ]]\n",
      "V = \n",
      " [[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n",
      "USV^t = \n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# U is self-explanatory\n",
    "U = linalg.svd(A)[0]\n",
    "\n",
    "# Looking at the documentation, S is the vector of singular values. \n",
    "# We actually want the matrix with appropriate dimension.  Using np.diag(linalg.svd(A)[1]) gives us a 2x2 matrix. \n",
    "# To make it 2 x 3 we need to pad that with a row of zeros at the bottom.\n",
    "\n",
    "S = np.pad(np.diag(v=linalg.svd(A)[1]),pad_width=[(0,1),(0,0)], mode='constant', constant_values=0)\n",
    "\n",
    "# The method returns V tranpose, not V.\n",
    "\n",
    "V = linalg.svd(A)[2].transpose()\n",
    "\n",
    "# Check that it works:\n",
    "\n",
    "print(\"U = \\n\", U)\n",
    "print(\"S = \\n\", S)\n",
    "print(\"V = \\n\", V)\n",
    "print(\"USV^t = \\n\", U @ S @ V.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank 1 approximation of $A$ is just $U \\Sigma V^\\top$, but replacing the smaller of the two eigenvalues with $0$.  So our rank $1$ approximation of $A$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.35662819 1.71846235]\n",
      " [3.09719707 3.92326845]\n",
      " [4.83776596 6.12807454]]\n"
     ]
    }
   ],
   "source": [
    "rank_1_approx = U  @ (S * [1,0]) @ V.transpose()\n",
    "print(rank_1_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that rank_1_approx is close to $A$, but its second column is a multiple of its first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These values should all be the same: [1.26671579 1.26671579 1.26671579]\n"
     ]
    }
   ],
   "source": [
    "print('These values should all be the same:', rank_1_approx[:,1]/rank_1_approx[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_spring_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
